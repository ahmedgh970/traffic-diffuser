{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T13:15:40.557224Z",
     "start_time": "2024-07-02T13:15:40.552760Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from scenarionet import read_dataset_summary, read_scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuscenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read scenario dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuscenes_pkl_dir = '/data/tii/data/nuscenes/nuscenes_trainval_pkl'\n",
    "dataset_summary, scenario_ids, mapping = read_dataset_summary(dataset_path=nuscenes_pkl_dir)\n",
    "\n",
    "scenario = read_scenario(dataset_path=nuscenes_pkl_dir, mapping=mapping, scenario_file_name=scenario_ids[0])\n",
    "\n",
    "#--- scenario keys\n",
    "print('Scenario keys : ', scenario.keys())\n",
    "\n",
    "#--- agent tracks\n",
    "print('\\n===== Agent tracks =====') \n",
    "print('Scenario tracks keys <=> Agents ids : ', scenario['tracks'].keys())\n",
    "print('Agent 6eac5d14e53743079cd2008e74aec375 keys : ', scenario['tracks']['6eac5d14e53743079cd2008e74aec375'].keys())\n",
    "print('Agent 6eac5d14e53743079cd2008e74aec375 state keys : ', scenario['tracks']['6eac5d14e53743079cd2008e74aec375']['state'].keys())\n",
    "#print(scenario['tracks']['6eac5d14e53743079cd2008e74aec375']['state']['position'])\n",
    "\n",
    "#--- map features\n",
    "print('\\n===== Map Features =====') \n",
    "print('Scenario map features keys : ', scenario['map_features'].keys())\n",
    "print('Map feature boundary_0 keys : ', scenario['map_features']['boundary_0'].keys())\n",
    "print('Map feature boundary_0 type : ', scenario['map_features']['boundary_0']['type'])\n",
    "#print(scenario['map_features']['boundary_0']['polyline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scenario(scenario_name, mapping, dataset_path, output_dir):\n",
    "    scenario = read_scenario(dataset_path=dataset_path, mapping=mapping, scenario_file_name=scenario_name)\n",
    "\n",
    "    scenario_list = []\n",
    "    for track_data in scenario['tracks'].values():\n",
    "        if track_data['type'] in {'PEDESTRIAN', 'CYCLIST', 'VEHICLE'}:\n",
    "            #vd = track_data['state']['valid']                #-- array of shape (seq_length,) (either 0. or 1.)\n",
    "            #hd = track_data['state']['heading']              #-- array of shape (seq_length,)\n",
    "            #v  = track_data['state']['velocity']             #-- array of shape (seq_length, 2)\n",
    "            #h  = track_data['state']['height']               #-- array of shape (seq_length, 1)\n",
    "            #w  = track_data['state']['width']                #-- array of shape (seq_length, 1)\n",
    "            #l  = track_data['state']['length']               #-- array of shape (seq_length, 1)\n",
    "            xyz = track_data['state']['position']            #-- array of shape (seq_length, 3)\n",
    "            agent = np.column_stack((xyz[:, 0], xyz[:, 1]))  # Modify with additional state parameters as needed\n",
    "            scenario_list.append(agent)\n",
    "\n",
    "    scenario_npy = np.array(scenario_list)\n",
    "\n",
    "    # Save processed scenario\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    np.save(os.path.join(output_dir, scenario_name.replace('.pkl', '.npy')), scenario_npy)\n",
    "    print(f'Scenario of shape {scenario_npy.shape} and id \"{scenario_name}\" has been saved!')\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset_path = '/data/tii/data/nuscenes_trainval_pkl/'\n",
    "    output_dir = '/data/tii/data/nuscenes_trainval_npy/'\n",
    "    _, scenario_ids, mapping = read_dataset_summary(dataset_path=dataset_path)\n",
    "    \n",
    "    for scenario_name in scenario_ids:\n",
    "        process_scenario(scenario_name, mapping, dataset_path, output_dir)       \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the plot\n",
    "# only polylines\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for k in scenario['map_features'].keys():\n",
    "    #if scenario['map_features'][k]['type'] in ['ROAD_LINE_BROKEN_SINGLE_WHITE', 'ROAD_LINE_SOLID_SINGLE_YELLOW', 'LANE_SURFACE_STREET', 'LANE_SURFACE_UNSTRUCTURE']:\n",
    "        for kk in scenario['map_features'][k].keys():\n",
    "            if kk == 'polyline':\n",
    "                polyline_arr = scenario['map_features'][k][kk]\n",
    "                # Plot the polyline\n",
    "                plt.plot(polyline_arr[:, 0], polyline_arr[:, 1], color='gray', linewidth=0.3)\n",
    "            elif kk == 'polygon':\n",
    "                polygon_arr = scenario['map_features'][k][kk]\n",
    "                # Plot the polygon\n",
    "                plt.plot(polygon_arr[:, 0], polygon_arr[:, 1], color='gray', linewidth=0.3)\n",
    "\n",
    "tracks = scenario['tracks']\n",
    "for idx, (id, track) in enumerate(tracks.items()):\n",
    "    #print(id)\n",
    "    #-- ('PEDESTRIAN', 'CYCLIST', 'TRAFFIC_CONE', 'TRAFFIC_BARRIER', 'VEHICLE')\n",
    "    if track['type'] in ['PEDESTRIAN', 'CYCLIST', 'VEHICLE']:\n",
    "        xyz = track['state']['position']  #-- array of shape (seq_length, 3)\n",
    "        #print(xyz.shape)\n",
    "        # Filter out invalid points\n",
    "        valid_points = xyz[(xyz[:, 0] != 0.0) & (xyz[:, 1] != 0.0)]\n",
    "        if valid_points.shape[0] > 0:\n",
    "            plt.plot(valid_points[:, 0], valid_points[:, 1], color='b')\n",
    "        \n",
    "\n",
    "# Remove axes\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map1 = np.load('/data/tii/data/nuscenes_maps/nuscenes_trainval_maps_norm_npy/sd_nuscenes_v1.0-trainval_scene-0160.npy')\n",
    "data1 = np.load('/data/tii/data/nuscenes_trainval_clean_train/sd_nuscenes_v1.0-trainval_scene-0160.npy')\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for ag in range(map1.shape[0]):\n",
    "    agent_traj = map1[ag, :, :]\n",
    "    plt.plot(agent_traj[:, 0], agent_traj[:, 1], color='gray', linewidth=0.3)\n",
    "\n",
    "for ag in range(data1.shape[0]):\n",
    "    agent_traj = data1[ag, :, :]\n",
    "    valid_xy = agent_traj[(agent_traj[:, 0] != 0.0) & (agent_traj[:, 1] != 0.0)]\n",
    "    plt.plot(valid_xy[:, 0], valid_xy[:, 1], color='b')\n",
    "    \n",
    "plt.show()\n",
    "plt.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_to_fixed_length(trajectory, num_timesteps, kind):\n",
    "    \"\"\"\n",
    "    Interpolate a variable-length trajectory to a fixed number of timesteps.\n",
    "\n",
    "    Parameters:\n",
    "    trajectory (np.ndarray): Input trajectory of shape (L, 2), where L is the number of timesteps.\n",
    "    num_timesteps (int): The number of timesteps to interpolate to (default is 10).\n",
    "    kind: choose from 'linear', 'quadratic', 'cubic'.\n",
    "    Returns:\n",
    "    np.ndarray: Interpolated trajectory of shape (num_timesteps, 2).\n",
    "    \"\"\"\n",
    "    L = trajectory.shape[0]  # Original length of the trajectory\n",
    "\n",
    "    # Original timesteps\n",
    "    original_timesteps = np.linspace(0, L - 1, L)\n",
    "\n",
    "    # New timesteps to interpolate to\n",
    "    new_timesteps = np.linspace(0, L - 1, num_timesteps)\n",
    "\n",
    "    # Interpolated trajectory\n",
    "    interpolated_trajectory = np.zeros((num_timesteps, 2))\n",
    "\n",
    "    # Interpolate x and y separately\n",
    "    for i in range(2):  # for each dimension x and y\n",
    "        interp_func = interp1d(original_timesteps, trajectory[:, i], kind=kind, fill_value='extrapolate')\n",
    "        interpolated_trajectory[:, i] = interp_func(new_timesteps)\n",
    "\n",
    "    return interpolated_trajectory\n",
    "\n",
    "\n",
    "def slice_array_based_on_condition(arr, epsilon):\n",
    "    slices = []\n",
    "    start_idx = 0\n",
    "\n",
    "    # Iterate through the array to find break points\n",
    "    for i in range(len(arr) - 1):\n",
    "        x_diff = abs(arr[i, 0] - arr[i + 1, 0])\n",
    "        y_diff = abs(arr[i, 1] - arr[i + 1, 1])\n",
    "\n",
    "        if x_diff > epsilon or y_diff > epsilon:\n",
    "            # If the condition is met, slice the array\n",
    "            slices.append(arr[start_idx:i+1])\n",
    "            start_idx = i + 1\n",
    "\n",
    "    # Add the last slice\n",
    "    if start_idx < len(arr):\n",
    "        slices.append(arr[start_idx:])\n",
    "    \n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sc in scenario_ids[0]:\n",
    "    print(f'Map of the scenario {sc} is being processed...')\n",
    "    scenario = read_scenario(dataset_path=nuscenes_pkl_dir, mapping=mapping, scenario_file_name=sc)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Determine the min max position of agents in the scenario \n",
    "    tracks = scenario['tracks']\n",
    "    min_x, min_y = float('inf'), float('inf')\n",
    "    max_x, max_y = 0., 0.\n",
    "    for idx, (id, track) in enumerate(tracks.items()):\n",
    "        xyz = track['state']['position']  #-- array of shape (seq_length, 3)\n",
    "        # Filter out invalid points\n",
    "        valid_points = xyz[(xyz[:, 0] != 0.0) & (xyz[:, 1] != 0.0)]\n",
    "        if valid_points.shape[0] > 0:\n",
    "            if max(valid_points[:, 0]) > max_x:\n",
    "                max_x = max(valid_points[:, 0])\n",
    "            if max(valid_points[:, 1]) > max_y:\n",
    "                max_y = max(valid_points[:, 1])\n",
    "            if min(valid_points[:, 0]) < min_x:\n",
    "                min_x = min(valid_points[:, 0])\n",
    "            if min(valid_points[:, 1]) < min_y:\n",
    "                min_y = min(valid_points[:, 1])\n",
    "        # plot the agents traj\n",
    "        plt.plot(valid_points[:, 0], valid_points[:, 1], color='b')\n",
    "\n",
    "    # Crop the map feature of the scenario and filter undesired points \n",
    "    arr_list = []\n",
    "    for k in scenario['map_features'].keys():\n",
    "        for kk in scenario['map_features'][k].keys():\n",
    "            if kk in ['polyline', 'polygon']:\n",
    "                arr = scenario['map_features'][k][kk]\n",
    "                arr = arr[:, :2]\n",
    "                if arr.shape[0]>1 :\n",
    "                    # interpolate \n",
    "                    arr = interpolate_to_fixed_length(arr, num_timesteps=50000, kind='linear')\n",
    "                    # crop to max_x max_y\n",
    "                    cropped_arr = arr[\n",
    "                        (arr[:, 0] >= min_x - 50) & \n",
    "                        (arr[:, 0] <= max_x + 50) &\n",
    "                        (arr[:, 1] >= min_y - 50) &\n",
    "                        (arr[:, 1] <= max_y + 50)]\n",
    "                    \n",
    "                    if cropped_arr.shape[0] > 1:\n",
    "                        # slice the cropped array to avoid weird cnx\n",
    "                        cropped_arr_slices = slice_array_based_on_condition(cropped_arr, epsilon=50)\n",
    "                        for arr_slice in cropped_arr_slices:\n",
    "                            if arr_slice.shape[0] > 1:\n",
    "                                # interpolate\n",
    "                                arr_slice = interpolate_to_fixed_length(arr_slice, num_timesteps=128, kind='linear')\n",
    "                                arr_list.append(arr_slice)\n",
    "    \n",
    "    arr_map = np.array(arr_list)\n",
    "    print(f'Shape of the map array: {arr_map.shape}')\n",
    "    \n",
    "    # plot the map\n",
    "    for idx in range(arr_map.shape[0]):\n",
    "        plt.plot(arr_map[idx, :, 0], arr_map[idx, :, 1], color='gray', linewidth=0.3)\n",
    "    \n",
    "    #dir_path = '/data/tii/data/nuscenes_maps/nuscenes_trainval_maps_npy/'\n",
    "    #os.makedirs(dir_path, exist_ok=True)\n",
    "    #full_path = os.path.join(dir_path, sc)\n",
    "    #new_file_path = full_path.replace('.pkl', '.npy')\n",
    "    #np.save(new_file_path, arr_map)\n",
    "    print(f'Map of the scenario {sc} saved in npy format!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_npy_files(directory):\n",
    "    \"\"\"List all .npy files in the specified directory.\"\"\"\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.npy')]\n",
    "\n",
    "def normalize_and_standardize_excluding_padding(data, mean, std):\n",
    "    \"\"\"Normalize and standardize the positions x and y in the data, excluding zero padding.\"\"\"\n",
    "    mask = np.any(data != 0, axis=-1)\n",
    "    normalized_standardized_data = np.copy(data)\n",
    "    normalized_standardized_data[mask] = (data[mask] - mean) / std\n",
    "    return normalized_standardized_data\n",
    "\n",
    "def process_grouped_npy_files(input_directory, output_directory, mean, std):\n",
    "    \"\"\"Process each grouped npy file to normalize and standardize the positions and save the modified files.\"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    npy_files = list_npy_files(input_directory)\n",
    "\n",
    "    for file_path in npy_files:\n",
    "        data = np.load(file_path)\n",
    "        normalized_standardized_data = normalize_and_standardize_excluding_padding(data, mean, std)\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        np.save(output_path, normalized_standardized_data)\n",
    "        print(f\"Processed and saved {output_path}\")\n",
    "\n",
    "def main():\n",
    "    #input_directory = '/data/tii/data/nuscenes_maps/nuscenes_trainval_maps_npy'\n",
    "    #output_directory = '/data/tii/data/nuscenes_maps/nuscenes_trainval_maps_norm_npy'\n",
    "    \n",
    "    mean = [1117.28378752, 1231.38483692]\n",
    "    std = [600.29805033, 427.54111417]\n",
    "    print(f'mean and std from padded trainval: (mean: {mean}, std: {std})')\n",
    "    \n",
    "    # Process files to normalize and standardize\n",
    "    process_grouped_npy_files(input_directory, output_directory, mean, std)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_npy_files(directory):\n",
    "    \"\"\"List all .npy files in the specified directory.\"\"\"\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.npy')]\n",
    "\n",
    "def process_grouped_npy_files(input_directory, output_directory, scale_factor):\n",
    "    \"\"\"Process each grouped npy file to normalize and standardize the positions and save the modified files.\"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    npy_files = list_npy_files(input_directory)\n",
    "\n",
    "    for file_path in npy_files:\n",
    "        data = np.load(file_path)\n",
    "        scaled_data = data * scale_factor\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        np.save(output_path, scaled_data)\n",
    "        print(f\"Processed and saved {output_path}\")\n",
    "\n",
    "def main():\n",
    "    #input_directory = '/data/tii/data/nuscenes_maps/nuscenes_trainval_maps_norm_npy'\n",
    "    #output_directory = '/data/tii/data/nuscenes_maps/nuscenes_trainval_maps_norm_npy'\n",
    "\n",
    "    # Process files to normalize and standardize\n",
    "    process_grouped_npy_files(input_directory, output_directory, scale_factor=100)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waymo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read scenario dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waymo_pkl_dir = '/data/tii/data/waymo/pkl'\n",
    "dataset_summary, scenario_ids, mapping = read_dataset_summary(dataset_path=waymo_pkl_dir)\n",
    "\n",
    "scenario = read_scenario(dataset_path=waymo_pkl_dir, mapping=mapping, scenario_file_name=scenario_ids[0])\n",
    "print('Scenario keys : ', scenario.keys())\n",
    "\n",
    "#--- agent tracks\n",
    "print('\\n===== Agent tracks =====') \n",
    "print('Scenario tracks keys <=> Agents ids : ', scenario['tracks'].keys())\n",
    "print('Agent 0 keys : ', scenario['tracks']['0'].keys())\n",
    "print('Agent 0 state keys : ', scenario['tracks']['0']['state'].keys())\n",
    "#print(scenario['tracks']['0']['state']['position'])\n",
    "\n",
    "#--- map features\n",
    "print('\\n===== Map Features =====') \n",
    "print('Scenario map features keys : ', scenario['map_features'].keys())\n",
    "print('Map feature 2 keys : ', scenario['map_features']['2'].keys())\n",
    "print('Map feature 2 type : ', scenario['map_features']['2']['type'])\n",
    "#print(scenario['map_features']['2']['polyline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scenario(scenario_name, mapping, dataset_path, output_dir):\n",
    "    scenario = read_scenario(dataset_path=dataset_path, mapping=mapping, scenario_file_name=scenario_name)\n",
    "\n",
    "    scenario_list = []\n",
    "    for track_data in scenario['tracks'].values():\n",
    "        if track_data['type'] in {'PEDESTRIAN', 'CYCLIST', 'VEHICLE'}:\n",
    "            xyz = track_data['state']['position'] \n",
    "            agent = np.column_stack((xyz[:, 0], xyz[:, 1]))  # Modify with additional state parameters as needed\n",
    "            scenario_list.append(agent)\n",
    "\n",
    "    scenario_npy = np.array(scenario_list)\n",
    "\n",
    "    # Save processed scenario\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    np.save(os.path.join(output_dir, scenario_name.replace('.pkl', '.npy')), scenario_npy)\n",
    "    print(f'Scenario of shape {scenario_npy.shape} and id \"{scenario_name}\" has been saved!')\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset_path = '/data/tii/data/waymo/pkl/'\n",
    "    output_dir = '/data/tii/data/waymo/npy/'\n",
    "    _, scenario_ids, mapping = read_dataset_summary(dataset_path=dataset_path)\n",
    "    \n",
    "    for scenario_name in scenario_ids:\n",
    "        process_scenario(scenario_name, mapping, dataset_path, output_dir)       \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the plot\n",
    "# only polylines\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for k in scenario['map_features'].keys():\n",
    "    #if scenario['map_features'][k]['type'] in ['ROAD_LINE_BROKEN_SINGLE_WHITE', 'ROAD_LINE_SOLID_SINGLE_YELLOW', 'LANE_SURFACE_STREET', 'LANE_SURFACE_UNSTRUCTURE']:\n",
    "        for kk in scenario['map_features'][k].keys():\n",
    "            if kk == 'polyline':\n",
    "                polyline_arr = scenario['map_features'][k][kk]\n",
    "                # Plot the polyline\n",
    "                plt.plot(polyline_arr[:, 0], polyline_arr[:, 1], color='gray', linewidth=0.3)\n",
    "            elif kk == 'polygon':\n",
    "                polygon_arr = scenario['map_features'][k][kk]\n",
    "                # Plot the polygon\n",
    "                plt.plot(polygon_arr[:, 0], polygon_arr[:, 1], color='gray', linewidth=0.3)\n",
    "\n",
    "tracks = scenario['tracks']\n",
    "for idx, (id, track) in enumerate(tracks.items()):\n",
    "    #print(id)\n",
    "    #-- ('PEDESTRIAN', 'CYCLIST', 'TRAFFIC_CONE', 'TRAFFIC_BARRIER', 'VEHICLE')\n",
    "    if track['type'] in ['PEDESTRIAN', 'CYCLIST', 'VEHICLE']:\n",
    "        xyz = track['state']['position']  #-- array of shape (seq_length, 3)\n",
    "        # Filter out invalid points\n",
    "        valid_points = xyz[(xyz[:, 0] != 0.0) & (xyz[:, 1] != 0.0)]\n",
    "        if valid_points.shape[0] > 0:\n",
    "            plt.plot(valid_points[:, 0], valid_points[:, 1], color='b')\n",
    "        \n",
    "\n",
    "# Remove axes\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_to_fixed_length(trajectory, num_timesteps, kind):\n",
    "    \"\"\"\n",
    "    Interpolate a variable-length trajectory to a fixed number of timesteps.\n",
    "\n",
    "    Parameters:\n",
    "    trajectory (np.ndarray): Input trajectory of shape (L, 2), where L is the number of timesteps.\n",
    "    num_timesteps (int): The number of timesteps to interpolate to (default is 10).\n",
    "    kind: choose from 'linear', 'quadratic', 'cubic'.\n",
    "    Returns:\n",
    "    np.ndarray: Interpolated trajectory of shape (num_timesteps, 2).\n",
    "    \"\"\"\n",
    "    L = trajectory.shape[0]  # Original length of the trajectory\n",
    "\n",
    "    # Original timesteps\n",
    "    original_timesteps = np.linspace(0, L - 1, L)\n",
    "\n",
    "    # New timesteps to interpolate to\n",
    "    new_timesteps = np.linspace(0, L - 1, num_timesteps)\n",
    "\n",
    "    # Interpolated trajectory\n",
    "    interpolated_trajectory = np.zeros((num_timesteps, 2))\n",
    "\n",
    "    # Interpolate x and y separately\n",
    "    for i in range(2):  # for each dimension x and y\n",
    "        interp_func = interp1d(original_timesteps, trajectory[:, i], kind=kind, fill_value='extrapolate')\n",
    "        interpolated_trajectory[:, i] = interp_func(new_timesteps)\n",
    "\n",
    "    return interpolated_trajectory\n",
    "\n",
    "\n",
    "def slice_array_based_on_condition(arr, epsilon):\n",
    "    slices = []\n",
    "    start_idx = 0\n",
    "\n",
    "    # Iterate through the array to find break points\n",
    "    for i in range(len(arr) - 1):\n",
    "        x_diff = abs(arr[i, 0] - arr[i + 1, 0])\n",
    "        y_diff = abs(arr[i, 1] - arr[i + 1, 1])\n",
    "\n",
    "        if x_diff > epsilon or y_diff > epsilon:\n",
    "            # If the condition is met, slice the array\n",
    "            slices.append(arr[start_idx:i+1])\n",
    "            start_idx = i + 1\n",
    "\n",
    "    # Add the last slice\n",
    "    if start_idx < len(arr):\n",
    "        slices.append(arr[start_idx:])\n",
    "    \n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_list = []\n",
    "for sc in scenario_ids[:3]:\n",
    "    print(f'Map of the scenario {sc} is being processed...')\n",
    "    scenario = read_scenario(dataset_path=waymo_pkl_dir, mapping=mapping, scenario_file_name=sc)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Determine the min max position of agents in the scenario \n",
    "    tracks = scenario['tracks']\n",
    "    min_x, min_y = float('inf'), float('inf')\n",
    "    max_x, max_y = 0., 0.\n",
    "    for idx, (id, track) in enumerate(tracks.items()):\n",
    "        xyz = track['state']['position']  #-- array of shape (seq_length, 3)\n",
    "        # Filter out invalid points\n",
    "        valid_points = xyz[(xyz[:, 0] != 0.0) & (xyz[:, 1] != 0.0)]\n",
    "        if valid_points.shape[0] > 0:\n",
    "            if max(valid_points[:, 0]) > max_x:\n",
    "                max_x = max(valid_points[:, 0])\n",
    "            if max(valid_points[:, 1]) > max_y:\n",
    "                max_y = max(valid_points[:, 1])\n",
    "            if min(valid_points[:, 0]) < min_x:\n",
    "                min_x = min(valid_points[:, 0])\n",
    "            if min(valid_points[:, 1]) < min_y:\n",
    "                min_y = min(valid_points[:, 1])\n",
    "        # plot the agents traj\n",
    "        plt.plot(valid_points[:, 0], valid_points[:, 1], color='b')\n",
    "\n",
    "    # Crop the map feature of the scenario and filter undesired points \n",
    "    arr_list = []\n",
    "    for k in scenario['map_features'].keys():\n",
    "        for kk in scenario['map_features'][k].keys():\n",
    "            if kk in ['polyline', 'polygon']:\n",
    "                arr = scenario['map_features'][k][kk]\n",
    "                arr = arr[:, :2]\n",
    "                if arr.shape[0]>1 :\n",
    "                    # interpolate \n",
    "                    arr = interpolate_to_fixed_length(arr, num_timesteps=50000, kind='linear')\n",
    "                    # crop to max_x max_y\n",
    "                    cropped_arr = arr[\n",
    "                        (arr[:, 0] >= min_x - 50) & \n",
    "                        (arr[:, 0] <= max_x + 50) &\n",
    "                        (arr[:, 1] >= min_y - 50) &\n",
    "                        (arr[:, 1] <= max_y + 50)]\n",
    "                    \n",
    "                    if cropped_arr.shape[0] > 1:\n",
    "                        # slice the cropped array to avoid weird cnx\n",
    "                        cropped_arr_slices = slice_array_based_on_condition(cropped_arr, epsilon=50)\n",
    "                        for arr_slice in cropped_arr_slices:\n",
    "                            if arr_slice.shape[0] > 1:\n",
    "                                # interpolate\n",
    "                                arr_slice = interpolate_to_fixed_length(arr_slice, num_timesteps=128, kind='linear')\n",
    "                                arr_list.append(arr_slice)\n",
    "    \n",
    "    arr_map = np.array(arr_list)\n",
    "    print(f'Shape of the map array: {arr_map.shape}')\n",
    "    \n",
    "    #-- plot the map\n",
    "    for idx in range(arr_map.shape[0]):\n",
    "        plt.plot(arr_map[idx, :, 0], arr_map[idx, :, 1], color='gray')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    #dir_path = '/data/tii/data/waymo/maps/npy'\n",
    "    #os.makedirs(dir_path, exist_ok=True)\n",
    "    #full_path = os.path.join(dir_path, sc)\n",
    "    #new_file_path = full_path.replace('.pkl', '.npy')\n",
    "    #np.save(new_file_path, arr_map)\n",
    "    print(f'Map of the scenario {sc} saved in npy format!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_npy_files(directory):\n",
    "    \"\"\"List all .npy files in the specified directory.\"\"\"\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.npy')]\n",
    "\n",
    "def normalize_and_standardize_excluding_padding(data, mean, std):\n",
    "    \"\"\"Normalize and standardize the positions x and y in the data, excluding zero padding.\"\"\"\n",
    "    mask = np.any(data != 0, axis=-1)\n",
    "    normalized_standardized_data = np.copy(data)\n",
    "    normalized_standardized_data[mask] = (data[mask] - mean) / std\n",
    "    return normalized_standardized_data\n",
    "\n",
    "def process_grouped_npy_files(input_directory, output_directory, mean, std):\n",
    "    \"\"\"Process each grouped npy file to normalize and standardize the positions and save the modified files.\"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    npy_files = list_npy_files(input_directory)\n",
    "\n",
    "    for file_path in npy_files:\n",
    "        data = np.load(file_path)\n",
    "        normalized_standardized_data = normalize_and_standardize_excluding_padding(data, mean, std)\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        np.save(output_path, normalized_standardized_data)\n",
    "        print(f\"Processed and saved {output_path}\")\n",
    "\n",
    "def main():\n",
    "    #input_directory = '/data/tii/data/waymo/maps/npy/'\n",
    "    #output_directory = '/data/tii/data/waymo/maps/norm_npy/'\n",
    "    \n",
    "    #-- modify these values with the mean and std from the padded scenarios\n",
    "    mean = [1645.6687,  568.7339]\n",
    "    std = [5140.911, 6306.616]\n",
    "    print(f'mean and std from padded trainval: (mean: {mean}, std: {std})')\n",
    "    \n",
    "    # Process files to normalize and standardize\n",
    "    process_grouped_npy_files(input_directory, output_directory, mean, std)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_npy_files(directory):\n",
    "    \"\"\"List all .npy files in the specified directory.\"\"\"\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.npy')]\n",
    "\n",
    "def process_grouped_npy_files(input_directory, output_directory, scale_factor):\n",
    "    \"\"\"Process each grouped npy file to normalize and standardize the positions and save the modified files.\"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    npy_files = list_npy_files(input_directory)\n",
    "\n",
    "    for file_path in npy_files:\n",
    "        data = np.load(file_path)\n",
    "        scaled_data = data * scale_factor\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        np.save(output_path, scaled_data)\n",
    "        print(f\"Processed and saved {output_path}\")\n",
    "\n",
    "def main():\n",
    "    #input_directory = '/data/tii/data/waymo/maps/norm_npy/'\n",
    "    #output_directory = '/data/tii/data/waymo/maps/norm_npy/'\n",
    "\n",
    "    # Process files to normalize and standardize\n",
    "    process_grouped_npy_files(input_directory, output_directory, scale_factor=100)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argoverse 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read scenario dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argoverse_pkl_dir = '/data/tii/data/argoverse/pkl/train_pkl'\n",
    "dataset_summary, scenario_ids, mapping = read_dataset_summary(dataset_path=argoverse_pkl_dir)\n",
    "\n",
    "scenario = read_scenario(dataset_path=argoverse_pkl_dir, mapping=mapping, scenario_file_name=scenario_ids[0])\n",
    "\n",
    "#--- scenario keys\n",
    "print('Scenario keys : ', scenario.keys())\n",
    "print('Scenario length : ', scenario['length'])\n",
    "\n",
    "#--- agent tracks\n",
    "print('\\n===== Agent tracks =====') \n",
    "print('Scenario tracks keys <=> Agents ids : ', scenario['tracks'].keys())\n",
    "print('Agent 50513 keys : ', scenario['tracks']['50513'].keys())\n",
    "print('Agent 50513 state keys : ', scenario['tracks']['50513']['state'].keys())\n",
    "#print(scenario['tracks']['50513']['state']['position'])\n",
    "\n",
    "#--- map features\n",
    "print('\\n===== Map Features =====') \n",
    "print('Scenario map features keys : ', scenario['map_features'].keys())\n",
    "print('Map feature 781506721 keys : ', scenario['map_features']['781506721'].keys())\n",
    "print('Map feature 781506721 type : ', scenario['map_features']['781506721']['type'])\n",
    "#print(scenario['map_features']['781506721']['polyline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scenario(scenario_name, mapping, dataset_path, output_dir):\n",
    "    scenario = read_scenario(dataset_path=dataset_path, mapping=mapping, scenario_file_name=scenario_name)\n",
    "\n",
    "    scenario_list = []\n",
    "    for track_data in scenario['tracks'].values():\n",
    "        if track_data['type'] in {'PEDESTRIAN', 'CYCLIST', 'VEHICLE'}:\n",
    "            xyz = track_data['state']['position'] \n",
    "            agent = np.column_stack((xyz[:, 0], xyz[:, 1]))  # Modify with additional state parameters as needed\n",
    "            scenario_list.append(agent)\n",
    "\n",
    "    scenario_npy = np.array(scenario_list)\n",
    "\n",
    "    # Save processed scenario\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    np.save(os.path.join(output_dir, scenario_name.replace('.pkl', '.npy')), scenario_npy)\n",
    "    print(f'Scenario of shape {scenario_npy.shape} and id \"{scenario_name}\" has been saved!')\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset_path = '/data/tii/data/argoverse/pkl/val_pkl/'\n",
    "    output_dir = '/data/tii/data/argoverse/npy/val_npy/'\n",
    "    _, scenario_ids, mapping = read_dataset_summary(dataset_path=dataset_path)\n",
    "    \n",
    "    for scenario_name in scenario_ids:\n",
    "        process_scenario(scenario_name, mapping, dataset_path, output_dir)       \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = read_scenario(dataset_path=argoverse_pkl_dir, mapping=mapping, scenario_file_name=scenario_ids[4])\n",
    "\n",
    "# Initialize the plot\n",
    "# only polylines\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for k in scenario['map_features'].keys():\n",
    "    #if scenario['map_features'][k]['type'] in ['ROAD_LINE_BROKEN_SINGLE_WHITE', 'ROAD_LINE_SOLID_SINGLE_YELLOW', 'LANE_SURFACE_STREET', 'LANE_SURFACE_UNSTRUCTURE']:\n",
    "        for kk in scenario['map_features'][k].keys():\n",
    "            if kk == 'polyline':\n",
    "                polyline_arr = scenario['map_features'][k][kk]\n",
    "                # Plot the polyline\n",
    "                plt.plot(polyline_arr[:, 0], polyline_arr[:, 1], color='gray', linewidth=0.3)\n",
    "            elif kk == 'polygon':\n",
    "                polygon_arr = scenario['map_features'][k][kk]\n",
    "                # Plot the polygon\n",
    "                plt.plot(polygon_arr[:, 0], polygon_arr[:, 1], color='gray', linewidth=0.3)\n",
    "\n",
    "tracks = scenario['tracks']\n",
    "for idx, (id, track) in enumerate(tracks.items()):\n",
    "    #print(id)\n",
    "    #-- ('PEDESTRIAN', 'CYCLIST', 'TRAFFIC_CONE', 'TRAFFIC_BARRIER', 'VEHICLE')\n",
    "    if track['type'] in ['PEDESTRIAN', 'CYCLIST', 'VEHICLE']:\n",
    "        xyz = track['state']['position']  #-- array of shape (seq_length, 3)\n",
    "        # Filter out invalid points\n",
    "        valid_points = xyz[(xyz[:, 0] != 0.0) & (xyz[:, 1] != 0.0)]\n",
    "        if valid_points.shape[0] > 0:\n",
    "            plt.plot(valid_points[:, 0], valid_points[:, 1], color='b')\n",
    "        \n",
    "\n",
    "# Remove axes\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_to_fixed_length(trajectory, num_timesteps, kind):\n",
    "    \"\"\"\n",
    "    Interpolate a variable-length trajectory to a fixed number of timesteps.\n",
    "\n",
    "    Parameters:\n",
    "    trajectory (np.ndarray): Input trajectory of shape (L, 2), where L is the number of timesteps.\n",
    "    num_timesteps (int): The number of timesteps to interpolate to (default is 10).\n",
    "    kind: choose from 'linear', 'quadratic', 'cubic'.\n",
    "    Returns:\n",
    "    np.ndarray: Interpolated trajectory of shape (num_timesteps, 2).\n",
    "    \"\"\"\n",
    "    L = trajectory.shape[0]  # Original length of the trajectory\n",
    "\n",
    "    # Original timesteps\n",
    "    original_timesteps = np.linspace(0, L - 1, L)\n",
    "\n",
    "    # New timesteps to interpolate to\n",
    "    new_timesteps = np.linspace(0, L - 1, num_timesteps)\n",
    "\n",
    "    # Interpolated trajectory\n",
    "    interpolated_trajectory = np.zeros((num_timesteps, 2))\n",
    "\n",
    "    # Interpolate x and y separately\n",
    "    for i in range(2):  # for each dimension x and y\n",
    "        interp_func = interp1d(original_timesteps, trajectory[:, i], kind=kind, fill_value='extrapolate')\n",
    "        interpolated_trajectory[:, i] = interp_func(new_timesteps)\n",
    "\n",
    "    return interpolated_trajectory\n",
    "\n",
    "\n",
    "def slice_array_based_on_condition(arr, epsilon):\n",
    "    slices = []\n",
    "    start_idx = 0\n",
    "\n",
    "    # Iterate through the array to find break points\n",
    "    for i in range(len(arr) - 1):\n",
    "        x_diff = abs(arr[i, 0] - arr[i + 1, 0])\n",
    "        y_diff = abs(arr[i, 1] - arr[i + 1, 1])\n",
    "\n",
    "        if x_diff > epsilon or y_diff > epsilon:\n",
    "            # If the condition is met, slice the array\n",
    "            slices.append(arr[start_idx:i+1])\n",
    "            start_idx = i + 1\n",
    "\n",
    "    # Add the last slice\n",
    "    if start_idx < len(arr):\n",
    "        slices.append(arr[start_idx:])\n",
    "    \n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sc in scenario_ids[:3]:\n",
    "    print(f'Map of the scenario {sc} is being processed...')\n",
    "    scenario = read_scenario(dataset_path=argoverse_pkl_dir, mapping=mapping, scenario_file_name=sc)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Determine the min max position of agents in the scenario \n",
    "    tracks = scenario['tracks']\n",
    "    min_x, min_y = float('inf'), float('inf')\n",
    "    max_x, max_y = 0., 0.\n",
    "    for idx, (id, track) in enumerate(tracks.items()):\n",
    "        xyz = track['state']['position']  #-- array of shape (seq_length, 3)\n",
    "        # Filter out invalid points\n",
    "        valid_points = xyz[(xyz[:, 0] != 0.0) & (xyz[:, 1] != 0.0)]\n",
    "        if valid_points.shape[0] > 0:\n",
    "            if max(valid_points[:, 0]) > max_x:\n",
    "                max_x = max(valid_points[:, 0])\n",
    "            if max(valid_points[:, 1]) > max_y:\n",
    "                max_y = max(valid_points[:, 1])\n",
    "            if min(valid_points[:, 0]) < min_x:\n",
    "                min_x = min(valid_points[:, 0])\n",
    "            if min(valid_points[:, 1]) < min_y:\n",
    "                min_y = min(valid_points[:, 1])\n",
    "        # plot the agents traj\n",
    "        plt.plot(valid_points[:, 0], valid_points[:, 1], color='b')\n",
    "\n",
    "    # Crop the map feature of the scenario and filter undesired points \n",
    "    arr_list = []\n",
    "    for k in scenario['map_features'].keys():\n",
    "        for kk in scenario['map_features'][k].keys():\n",
    "            if kk in ['polyline', 'polygon']:\n",
    "                arr = scenario['map_features'][k][kk]\n",
    "                arr = arr[:, :2]\n",
    "                if arr.shape[0]>1 :\n",
    "                    # interpolate \n",
    "                    arr = interpolate_to_fixed_length(arr, num_timesteps=50000, kind='linear')\n",
    "                    # crop to max_x max_y\n",
    "                    cropped_arr = arr[\n",
    "                        (arr[:, 0] >= min_x - 50) & \n",
    "                        (arr[:, 0] <= max_x + 50) &\n",
    "                        (arr[:, 1] >= min_y - 50) &\n",
    "                        (arr[:, 1] <= max_y + 50)]\n",
    "                    \n",
    "                    if cropped_arr.shape[0] > 1:\n",
    "                        # slice the cropped array to avoid weird cnx\n",
    "                        cropped_arr_slices = slice_array_based_on_condition(cropped_arr, epsilon=50)\n",
    "                        for arr_slice in cropped_arr_slices:\n",
    "                            if arr_slice.shape[0] > 1:\n",
    "                                # interpolate\n",
    "                                arr_slice = interpolate_to_fixed_length(arr_slice, num_timesteps=128, kind='linear')\n",
    "                                arr_list.append(arr_slice)\n",
    "    \n",
    "    arr_map = np.array(arr_list)\n",
    "    print(f'Shape of the map array: {arr_map.shape}')\n",
    "    \n",
    "    #-- plot the map\n",
    "    for idx in range(arr_map.shape[0]):\n",
    "        plt.plot(arr_map[idx, :, 0], arr_map[idx, :, 1], color='gray', linewidth=0.3)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    #dir_path = '/data/tii/data/waymo/maps/npy'\n",
    "    #os.makedirs(dir_path, exist_ok=True)\n",
    "    #full_path = os.path.join(dir_path, sc)\n",
    "    #new_file_path = full_path.replace('.pkl', '.npy')\n",
    "    #np.save(new_file_path, arr_map)\n",
    "    print(f'Map of the scenario {sc} saved in npy format!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_npy_files(directory):\n",
    "    \"\"\"List all .npy files in the specified directory.\"\"\"\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.npy')]\n",
    "\n",
    "def normalize_and_standardize_excluding_padding(data, mean, std):\n",
    "    \"\"\"Normalize and standardize the positions x and y in the data, excluding zero padding.\"\"\"\n",
    "    mask = np.any(data != 0, axis=-1)\n",
    "    normalized_standardized_data = np.copy(data)\n",
    "    normalized_standardized_data[mask] = (data[mask] - mean) / std\n",
    "    return normalized_standardized_data\n",
    "\n",
    "def process_grouped_npy_files(input_directory, output_directory, mean, std):\n",
    "    \"\"\"Process each grouped npy file to normalize and standardize the positions and save the modified files.\"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    npy_files = list_npy_files(input_directory)\n",
    "\n",
    "    for file_path in npy_files:\n",
    "        data = np.load(file_path)\n",
    "        normalized_standardized_data = normalize_and_standardize_excluding_padding(data, mean, std)\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        np.save(output_path, normalized_standardized_data)\n",
    "        print(f\"Processed and saved {output_path}\")\n",
    "\n",
    "def main():\n",
    "    #input_directory = '/data/tii/data/argoverse/maps/npy/'\n",
    "    #output_directory = '/data/tii/data/waymo/argoverse/norm_npy/'\n",
    "    \n",
    "    #-- modify these values with the mean and std from the padded scenarios\n",
    "    #-- train_npy\n",
    "    mean = [2486.6545, 1101.2592]\n",
    "    std = [2729.8386, 1496.0399]\n",
    "    #-- val_npy\n",
    "    mean = [2926.3154, 1114.3304]\n",
    "    std = [3138.867,  1613.2261]\n",
    "    \n",
    "    print(f'mean and std from padded trainval: (mean: {mean}, std: {std})')\n",
    "    \n",
    "    # Process files to normalize and standardize\n",
    "    process_grouped_npy_files(input_directory, output_directory, mean, std)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_npy_files(directory):\n",
    "    \"\"\"List all .npy files in the specified directory.\"\"\"\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.npy')]\n",
    "\n",
    "def process_grouped_npy_files(input_directory, output_directory, scale_factor):\n",
    "    \"\"\"Process each grouped npy file to normalize and standardize the positions and save the modified files.\"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    npy_files = list_npy_files(input_directory)\n",
    "\n",
    "    for file_path in npy_files:\n",
    "        data = np.load(file_path)\n",
    "        scaled_data = data * scale_factor\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        np.save(output_path, scaled_data)\n",
    "        print(f\"Processed and saved {output_path}\")\n",
    "\n",
    "def main():\n",
    "    #input_directory = '/data/tii/data/argoverse/maps/norm_npy/'\n",
    "    #output_directory = '/data/tii/data/argoverse/maps/norm_npy/'\n",
    "\n",
    "    # Process files to normalize and standardize\n",
    "    process_grouped_npy_files(input_directory, output_directory, scale_factor=100)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Sample the sequence length to 13. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_npy_files(directory):\n",
    "    \"\"\"List all .npy files in the specified directory.\"\"\"\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.npy')]\n",
    "\n",
    "def process_npy_files(input_directory, output_directory, step=12):\n",
    "    \"\"\"Process each grouped npy file to replace padded values and save the modified files.\"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    npy_files = list_npy_files(input_directory)\n",
    "\n",
    "    for file_path in npy_files:\n",
    "        data = np.load(file_path)\n",
    "        print('Data shape', data.shape)\n",
    "        data_sampled = data[:, ::step, :]\n",
    "        print('Sampled data shape: ', data_sampled.shape)\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        np.save(output_path, data_sampled)\n",
    "        print(f\"Cropped, sampled, and saved {output_path}\")\n",
    "\n",
    "def main():\n",
    "    input_directory = '/data/tii/data/argoverse/npy'\n",
    "    output_directory = '/data/tii/data/argoverse/npy_prep'\n",
    "\n",
    "    process_npy_files(input_directory, output_directory, step=9) # step=12 for nuscnes, 16 for waymo, and 9 for argoverse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Filter full padded agents and out-of-range scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_agents(scenario):\n",
    "    \"\"\"\n",
    "    Remove agents from the scenario where all values in all dimensions are zero across the sequence length.\n",
    "    \n",
    "    Parameters:\n",
    "    - scenario: numpy array of shape (N, L, D), where N is the number of agents, L is the sequence length, and D is the dimension.\n",
    "\n",
    "    Returns:\n",
    "    - Filtered numpy array with zero agents removed.\n",
    "    \"\"\"\n",
    "    # Identify agents where all values are zero across the entire sequence length and dimensions\n",
    "    non_zero_agents = ~np.all(scenario == 0.0, axis=(1, 2))  # Shape will be (N,), where True indicates non-zero agent\n",
    "    \n",
    "    # Filter out agents that are all-zero\n",
    "    filtered_scenario = scenario[non_zero_agents]\n",
    "    return filtered_scenario\n",
    "\n",
    "\n",
    "# Directory where scenarios are stored\n",
    "data_dir = '/data/tii/data/argoverse/npy_prep'\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    scenario = np.load(file_path)\n",
    "    \n",
    "    # Remove all-zero agents\n",
    "    filtered_scenario = remove_zero_agents(scenario)\n",
    "    \n",
    "    # Save the filtered scenario\n",
    "    output_path = os.path.join(data_dir, filename)\n",
    "    np.save(output_path, filtered_scenario)\n",
    "    \n",
    "    print(f\"Processed and saved {filename} with shape original shape {scenario.shape} and new shape {filtered_scenario.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where scenarios are stored\n",
    "#data_dir = '/data/tii/data/waymo/npy'\n",
    "#output_dir = '/data/tii/data/waymo/npy_'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set threshold for filtering scenarios (e.g., values in the y dimension should not exceed 81127)\n",
    "threshold = 81127\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    scenario = np.load(file_path)\n",
    "    \n",
    "    # Check if any value in the y dimension (index 1) exceeds the threshold\n",
    "    if np.any(scenario[:, :, 1] > threshold) or np.any(scenario[:, :, 1] < -threshold):\n",
    "        print(f'Scenario {filename} discarded due to out-of-range values')\n",
    "    else:\n",
    "        # Save the filtered scenario\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        np.save(output_path, scenario)\n",
    "        print(f'Scenario {filename} saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Filter stastionary agents from prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_movement(data_part):\n",
    "    # Mask for non-padded steps\n",
    "    non_padded_mask = ~(np.all(data_part == 0., axis=2))  \n",
    "    \n",
    "    # Calculate differences between consecutive positions\n",
    "    diffs = np.diff(data_part, axis=1) \n",
    "    \n",
    "    # Create valid mask for differences (True only if both steps are non-padded)\n",
    "    valid_diffs_mask = non_padded_mask[:, :-1] & non_padded_mask[:, 1:]\n",
    "    valid_diffs = diffs * valid_diffs_mask[:, :, np.newaxis]\n",
    "    \n",
    "    # Calculate Euclidean distance for each valid difference\n",
    "    distances = np.linalg.norm(valid_diffs, axis=2)\n",
    "    total_movement = distances.sum(axis=1)\n",
    "    return total_movement\n",
    "\n",
    "def remove_stationary_agents(data, min_distance):\n",
    "    \"\"\"\n",
    "    Remove stationary agents from a dataset while ignoring padded data.\n",
    "\n",
    "    Parameters:\n",
    "        data (numpy.ndarray): A dataset containing trajectories of agents.\n",
    "                              Expected shape is (N, L, D) where N is the number of agents,\n",
    "                              L is the sequence length, and D is the dimension (e.g., x, y positions).\n",
    "        min_distance (float): The minimum total distance an agent must move to not be considered stationary.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A new dataset with stationary agents removed.\n",
    "    \"\"\"\n",
    "    # Split data into history and future parts\n",
    "    data_history = data[:, :8, :]  # Shape: (N, 8, D)\n",
    "    data_future = data[:, 8:, :]   # Shape: (N, L-8, D)\n",
    "    \n",
    "    history_total_movement = calculate_total_movement(data_history)\n",
    "    future_total_movement = calculate_total_movement(data_future)\n",
    "    \n",
    "    # Identify non-stationary agents in both history and future\n",
    "    non_stationary_agents = (history_total_movement > min_distance) & (future_total_movement > min_distance)\n",
    "    \n",
    "    # Filter out the stationary agents\n",
    "    filtered_data = data[non_stationary_agents]\n",
    "\n",
    "    return filtered_data, history_total_movement, future_total_movement\n",
    "\n",
    "\n",
    "def main():\n",
    "    # set at the minimum distance travelled \n",
    "    # for historical and future trajectories per agent\n",
    "    min_distance=4\n",
    "    input_dir = '/data/tii/data/nuscenes/npy_prep'\n",
    "    output_dir = '/data/tii/data/nuscenes/npy_clean'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    history_total_movement_list, future_total_movement_list = [], []\n",
    "    for filename in os.listdir(input_dir):\n",
    "        data = np.load(os.path.join(input_dir, filename))\n",
    "        filtered_data, history_total_movement, future_total_movement = remove_stationary_agents(data, min_distance=min_distance)\n",
    "        \n",
    "        history_total_movement_list.append(history_total_movement)\n",
    "        future_total_movement_list.append(future_total_movement)\n",
    "        \n",
    "        if filtered_data.shape[0] != 0:  # Only save non-empty filtered data\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            np.save(output_path, filtered_data)\n",
    "            print(f\"Shape of the original data {data.shape} and the filtered data {filtered_data.shape}\")\n",
    "            print(f\"Processed and saved {output_path}\")\n",
    "        else:\n",
    "            print(f\"All agents are stationnary in {filename}, so the scenario wasn't retained\")\n",
    "    \n",
    "    #flattened_history = np.concatenate(history_total_movement_list)\n",
    "    #flattened_history = flattened_history[flattened_history > min_distance]\n",
    "    #flattened_future = np.concatenate(future_total_movement_list)\n",
    "    #flattened_future = flattened_future[flattened_future > min_distance]\n",
    "    #mean_future = np.mean(flattened_future)\n",
    "    #mean_history = np.mean(flattened_history)\n",
    "\n",
    "    #plt.figure(figsize=(10, 6))\n",
    "    #plt.hist(flattened_history, bins=10, edgecolor='g', alpha=0.3, label=\"History Movement\")\n",
    "    #plt.hist(flattened_future, bins=10, edgecolor='b', alpha=0.3, label=\"Future Movement\")\n",
    "\n",
    "    #plt.axvline(mean_history, color='g', linestyle='dashed', linewidth=1.5, label=f\"History Mean: {mean_history:.2f}\")\n",
    "    #plt.axvline(mean_future, color='b', linestyle='dashed', linewidth=1.5, label=f\"Future Mean: {mean_future:.2f}\")\n",
    "\n",
    "    #plt.title(\"Histogram of Total Movement for Future and History Trajectories\")\n",
    "    #plt.xlabel(\"Total Movement\")\n",
    "    #plt.ylabel(\"Frequency\")\n",
    "    #plt.legend()\n",
    "    #plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    #plt.show()\n",
    "    \n",
    "              \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_real_dir = '/data/tii/data/nuscenes/npy_clean'\n",
    "max_plots = 100\n",
    "figsize = (10, 10)\n",
    "#fig = plt.figure(figsize=figsize)\n",
    "for l_real in sorted(os.listdir(data_real_dir))[0:850:8]:\n",
    "    print(l_real)\n",
    "    data_real = np.load(os.path.join(data_real_dir, l_real))\n",
    "    data_hist, data_future = data_real[:, :8, :], data_real[:, 7:, :]\n",
    "    print(f'Real data has shape: {data_real.shape}')\n",
    "\n",
    "    # Loop through each scene and plot separately\n",
    "    plt.figure(figsize=figsize) \n",
    "    for ag in [i for i in range(data_hist.shape[0])]:  # Loop through each agent in the scene   \n",
    "        agent_hist, agent_future = data_hist[ag], data_future[ag] \n",
    "        \n",
    "        valid_agent_future = agent_future[(agent_future[:, 0] != 0.0) & (agent_future[:, 1] != 0.0)]\n",
    "        plt.plot(valid_agent_future[:, 0], valid_agent_future[:, 1], 'g-o', linewidth=1.2, markersize=2.8)\n",
    "        \n",
    "        valid_agent_hist = agent_hist[(agent_hist[:, 0] != 0.0) & (agent_hist[:, 1] != 0.0)]\n",
    "        plt.plot(valid_agent_hist[:, 0], valid_agent_hist[:, 1], 'k-o', linewidth=1.2, markersize=2.8)\n",
    "    plt.show() \n",
    "    plt.close()\n",
    "        \n",
    "#plt.show() \n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4- Standardize the x,y positions through a dataset specific mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dataset_statistics(input_dir):\n",
    "    \"\"\" \n",
    "    Calculate Mean/Std and Min/Max values for x and y positions \n",
    "    throughout the dataset, excluding zero padding.\n",
    "    \"\"\"\n",
    "    scenario_list = []\n",
    "    for filename in os.listdir(input_dir):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        scenario = np.load(file_path)\n",
    "        #scenario_list.append(scenario.reshape(-1, 2)) # if including padding \n",
    "        mask = np.any(scenario != 0, axis=-1)\n",
    "        scenario_nopad = scenario[mask]\n",
    "        scenario_list.append(scenario_nopad)    \n",
    "    print('The dataset contains', len(scenario_list), 'scenarios.')\n",
    "    dataset = np.concatenate(scenario_list, axis=0)\n",
    "    mean = np.mean(dataset, axis=0)\n",
    "    std = np.std(dataset, axis=0)\n",
    "    min = np.min(dataset, axis=0)\n",
    "    max = np.max(dataset, axis=0)\n",
    "    print(f'Mean: {mean}, Std: {std}, Min: {min}, Max: {max}')\n",
    "    return mean, std, min, max\n",
    " \n",
    "def normalize(data, min, max):\n",
    "    \"\"\"Normalize data xy, excluding padding.\"\"\"\n",
    "    mask = np.any(data != 0, axis=-1)\n",
    "    data_p = np.copy(data)\n",
    "    data_p[mask] = 2 * (data[mask] - min) / (max - min) -1\n",
    "    return data_p\n",
    "\n",
    "def standardize(data, mean, std):\n",
    "    \"\"\"Standardize data xy, excluding padding.\"\"\"\n",
    "    mask = np.any(data != 0, axis=-1)\n",
    "    data_p = np.copy(data)\n",
    "    data_p[mask] = (data[mask] - mean) / std\n",
    "    return data_p\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_dir = '/data/tii/data/argoverse/npy_clean'\n",
    "    output_dir = '/data/tii/data/argoverse/npy_stand'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Calculate global min, max, mean, and std\n",
    "    mean, std, min, max = calculate_dataset_statistics(input_dir)\n",
    "    \n",
    "    # Step 2: Process each scenario\n",
    "    for filename in os.listdir(input_dir):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        scenario = np.load(file_path)\n",
    "        \n",
    "        # Normalize scenario:\n",
    "        #scenario = normalize(scenario, min, max)\n",
    "        \n",
    "        # Standardize scenario:\n",
    "        scenario = standardize(scenario, mean, std)\n",
    "        \n",
    "        # Save processed scenario\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        np.save(output_path, scenario)\n",
    "        print(f'Scenario {filename} processed and saved.')\n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5- Scale the standardized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    scale_factor = 100  # choose the scale to avoid values near zero \n",
    "    #input_dir = '/data/tii/data/argoverse/3_npy_stand'\n",
    "    #output_dir = '/data/tii/data/argoverse/4_npy_scaled_100'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Process each scenario\n",
    "    for filename in os.listdir(input_dir):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        scenario = np.load(file_path)\n",
    "        \n",
    "        # Scale scenario:\n",
    "        scaled_scenario = scenario * scale_factor\n",
    "        \n",
    "        # Save processed scenario\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        np.save(output_path, scaled_scenario)\n",
    "        print(f'Scenario {filename} scaled and saved.')\n",
    "        \n",
    "    print(\"Scenarios scaling complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6- Visualize histogram of x or y positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = '/data/tii/data/waymo/npy_stand'\n",
    "\n",
    "# Initialize an empty list to accumulate y positions\n",
    "all_y_positions = []\n",
    "\n",
    "# Iterate over each file in the directory and accumulate y positions\n",
    "for filename in os.listdir(input_directory):\n",
    "    file_path = os.path.join(input_directory, filename)\n",
    "    data = np.load(file_path)\n",
    "    \n",
    "    # Extract y positions and append to the list\n",
    "    y_positions = data[:, :, 1].flatten()\n",
    "    all_y_positions.extend(y_positions)\n",
    "\n",
    "# Convert the accumulated y positions to a numpy array for plotting\n",
    "all_y_positions = np.array(all_y_positions)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_y_positions, bins=100, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram of Y Positions Across Dataset')\n",
    "plt.xlabel('Y Position')\n",
    "plt.ylabel('Frequency')\n",
    "plt.yscale('log')  # Use log scale for better visibility of distribution\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.7)\n",
    "plt.axvline(0, color='red', linestyle='--', label='Y = 0')\n",
    "plt.legend()\n",
    "\n",
    "# Set x-axis ticks to help visualize index range\n",
    "max_value = int(np.ceil(np.max(np.abs(all_y_positions))))\n",
    "plt.xticks(np.linspace(-max_value, max_value, num=10, endpoint=True))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7- train test split from main dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 674 files to /data/tii/data/autod/train_merged_scaled_100\n",
      "Copied 169 files to /data/tii/data/autod/test_merged_scaled_100\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(main_dir, train_dir, test_dir, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split files from a main dataset directory into train and test directories with a given ratio.\n",
    "\n",
    "    Parameters:\n",
    "        main_dir (str): Path to the main dataset directory containing the files.\n",
    "        train_dir (str): Path to the train directory where 80% of the files will be moved.\n",
    "        test_dir (str): Path to the test directory where 20% of the files will be moved.\n",
    "        train_ratio (float): Proportion of files to move to the train directory (default is 0.8 for 80%).\n",
    "    \"\"\"\n",
    "    # Get list of all files in the main directory\n",
    "    all_files = os.listdir(main_dir)\n",
    "    total_files = len(all_files)\n",
    "\n",
    "    # Shuffle the file list for randomness\n",
    "    random.shuffle(all_files)\n",
    "\n",
    "    # Calculate split index\n",
    "    split_index = int(total_files * train_ratio)\n",
    "\n",
    "    # Split files for train and test\n",
    "    train_files = all_files[:split_index]\n",
    "    test_files = all_files[split_index:]\n",
    "\n",
    "    # Create train and test directories if they don't exist\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Move files to train directory\n",
    "    for file_name in train_files:\n",
    "        shutil.copy(os.path.join(main_dir, file_name), os.path.join(train_dir, file_name))\n",
    "\n",
    "    # Move files to test directory\n",
    "    for file_name in test_files:\n",
    "        shutil.copy(os.path.join(main_dir, file_name), os.path.join(test_dir, file_name))\n",
    "\n",
    "    print(f\"Copied {len(train_files)} files to {train_dir}\")\n",
    "    print(f\"Copied {len(test_files)} files to {test_dir}\")\n",
    "\n",
    "# Example usage\n",
    "main_dir = \"/data/tii/data/nuscenes/4_npy_scaled_100\"\n",
    "train_dir = \"/data/tii/data/autod/train_merged_scaled_100\"\n",
    "test_dir = \"/data/tii/data/autod/test_merged_scaled_100\"\n",
    "split_dataset(main_dir, train_dir, test_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8- Pad scenarios to max number of agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of agents (N) across all files: 44\n",
      "Second maximum number of agents (N): 40\n",
      "Third maximum number of agents (N): 33\n",
      "Minimum number of agents (N) across all files: 1\n",
      "Second minimum number of agents (N): 2\n",
      "Third minimum number of agents (N): 3\n"
     ]
    }
   ],
   "source": [
    "# Paths to your dataset\n",
    "input_directory = '/data/tii/data/nuscenes/3_npy_stand'\n",
    "\n",
    "# Step 1: Determine the maximum, minimum, second minimum, and third minimum N\n",
    "max_N = 0\n",
    "min_N = float('inf')\n",
    "unique_N = set()  # To store unique N values\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.npy'):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        data = np.load(file_path)\n",
    "        N, L, D = data.shape\n",
    "        # Update max and min N\n",
    "        max_N = max(max_N, N)\n",
    "        min_N = min(min_N, N)\n",
    "        # Store the unique N values\n",
    "        unique_N.add(N)\n",
    "\n",
    "# Convert the set of unique N values to a sorted list\n",
    "sorted_N = sorted(unique_N)\n",
    "\n",
    "# Get the second and third minimum if they exist\n",
    "second_min_N = sorted_N[1] if len(sorted_N) > 1 else None\n",
    "third_min_N = sorted_N[2] if len(sorted_N) > 2 else None\n",
    "\n",
    "# Get the second and third maximum if they exist\n",
    "second_max_N = sorted_N[-2] if len(sorted_N) > 1 else None\n",
    "third_max_N = sorted_N[-3] if len(sorted_N) > 2 else None\n",
    "\n",
    "print(f\"Maximum number of agents (N) across all files: {max_N}\")\n",
    "print(f\"Second maximum number of agents (N): {second_max_N}\")\n",
    "print(f\"Third maximum number of agents (N): {third_max_N}\")\n",
    "\n",
    "print(f\"Minimum number of agents (N) across all files: {min_N}\")\n",
    "print(f\"Second minimum number of agents (N): {second_min_N}\")\n",
    "print(f\"Third minimum number of agents (N): {third_min_N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your dataset\n",
    "#input_directory = '/data/tii/data/argoverse/npy'\n",
    "#output_directory = '/data/tii/data/argoverse/npy_prep'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "max_N = 49\n",
    "# Step 2: Pad each file to max_N\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.npy'):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        data = np.load(file_path)\n",
    "        N, L, D = data.shape\n",
    "        \n",
    "        if N < max_N:\n",
    "            # Create a new array with the shape (max_N, L, D) and fill it with zeros\n",
    "            padded_data = np.zeros((max_N, L, D))\n",
    "            \n",
    "            # Copy the original data into the new array\n",
    "            padded_data[:N, :, :] = data\n",
    "            \n",
    "            # Save the padded data\n",
    "            output_path = os.path.join(output_directory, filename)\n",
    "            np.save(output_path, padded_data)\n",
    "        else:\n",
    "            # If no padding is needed, just copy the original file to the output directory\n",
    "            output_path = os.path.join(output_directory, filename)\n",
    "            np.save(output_path, data)\n",
    "\n",
    "print(\"Padding completed and saved to the output directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0- Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Move/Rename files from dir_A to dir_D knowing dir_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_matching_files(dir_A, dir_B, dir_D):\n",
    "    \"\"\"\n",
    "    Move files from dir_A to dir_D where the filenames match those in dir_B.\n",
    "\n",
    "    Parameters:\n",
    "    dir_A (str): The source directory from which to move files.\n",
    "    dir_B (str): The reference directory to check for matching filenames.\n",
    "    dir_D (str): The destination directory where files will be moved.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(dir_D, exist_ok=True)\n",
    "\n",
    "    # Get the set of filenames in dir_B\n",
    "    files_in_B = set(os.listdir(dir_B))\n",
    "\n",
    "    # Iterate over files in dir_A\n",
    "    for filename in os.listdir(dir_A):\n",
    "        if filename in files_in_B:\n",
    "            # If a matching file is found, construct full paths\n",
    "            source_path = os.path.join(dir_A, filename)\n",
    "            destination_path = os.path.join(dir_D, filename)\n",
    "\n",
    "            # Move the file from dir_A to dir_D\n",
    "            shutil.move(source_path, destination_path)\n",
    "            print(f\"Moved: {filename} from {dir_A} to {dir_D}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dir_A = '/data/tii/data/nuscenes/npy_train'  \n",
    "    dir_B = '/data/tii/data/nuscenes/npy_clean_test'  \n",
    "    dir_D = '/data/tii/data/nuscenes/npy_test'  \n",
    "\n",
    "    move_matching_files(dir_A, dir_B, dir_D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(dir_a, dir_b, dir_c):\n",
    "    # Get sorted list of files from both directories\n",
    "    files_a = sorted(os.listdir(dir_a))\n",
    "    files_b = sorted(os.listdir(dir_b))\n",
    "\n",
    "    # Check if both directories have the same number of files\n",
    "    if len(files_a) != len(files_b):\n",
    "        print(\"Error: The number of files in both directories must be the same.\")\n",
    "        return\n",
    "\n",
    "    # Rename files in directory B using names from directory A\n",
    "    for file_a, file_b in zip(files_a, files_b):\n",
    "        # Get full paths\n",
    "        path_a = os.path.join(dir_a, file_a)\n",
    "        path_b = os.path.join(dir_b, file_b)\n",
    "        \n",
    "        # Determine the new name for the file in B\n",
    "        destination_path = os.path.join(dir_c, file_a)\n",
    "        shutil.move(path_b, destination_path)\n",
    "        \n",
    "        # Moved the renamed file in C\n",
    "        print(f\"Renamed {file_b} to {file_a}\")\n",
    "\n",
    "# Example usage\n",
    "dir_a = '/data/tii/data/nuscenes_trainval_npy'\n",
    "dir_b = '/data/tii/data/nuscenes_maps/nuscenes_trainval_raster_npy_copy'\n",
    "dir_c = '/data/tii/data/nuscenes_maps/nuscenes_trainval_raster'\n",
    "\n",
    "os.makedirs(dir_c, exist_ok=True)\n",
    "rename_files(dir_a, dir_b, dir_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot npy and rasterized maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dir = '/data/tii/data/nuscenes_maps/nuscenes_trainval_maps_test'\n",
    "max_plots = 3\n",
    "figsize = (10, 10)\n",
    "for l in sorted(os.listdir(map_dir))[:max_plots]:\n",
    "    print(l)\n",
    "    data = np.load(os.path.join(map_dir, l))\n",
    "    # Loop through each scene and plot separately\n",
    "    plt.figure(figsize=figsize) \n",
    "    for ag in [i for i in range(data.shape[0])]:  # Loop through each agent in the scene   \n",
    "        plt.plot(data[ag, :, 0], data[ag, :, 1], 'gray', linewidth=0.8)\n",
    "    plt.show() \n",
    "    plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dir = '/data/tii/data/nuscenes_maps/nuscenes_trainval_raster_test'\n",
    "\n",
    "for l in sorted(os.listdir(map_dir))[:10]:\n",
    "    print(l)\n",
    "    data = np.load(os.path.join(map_dir, l))\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    for i in range(4):\n",
    "        axes[i].imshow(data[i], cmap='gray')  # Plot each slice in grayscale\n",
    "        axes[i].set_title(f'Image {i+1}')  # Set title for each subplot\n",
    "        axes[i].axis('off')  # Turn off axis labels and ticks\n",
    "\n",
    "    plt.show()  # Display the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace padded values with saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_npy_files(directory):\n",
    "    \"\"\"List all .npy files in the specified directory.\"\"\"\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.npy')]\n",
    "\n",
    "def replace_padded_values(data):\n",
    "    \"\"\"Replace padded values (0.0) with the first next or previous non-zero (x, y) values.\"\"\"\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            # If current value is zero, find the next or previous non-zero value\n",
    "            if np.all(data[i, j] == 0):\n",
    "                # Find next non-zero value\n",
    "                k = j + 1\n",
    "                while k < data.shape[1] and np.all(data[i, k] == 0):\n",
    "                    k += 1\n",
    "                \n",
    "                if k < data.shape[1]:\n",
    "                    data[i, j] = data[i, k]\n",
    "                else:\n",
    "                    # If no next non-zero value, find the previous non-zero value\n",
    "                    k = j - 1\n",
    "                    while k >= 0 and np.all(data[i, k] == 0):\n",
    "                        k -= 1\n",
    "                    \n",
    "                    if k >= 0:\n",
    "                        data[i, j] = data[i, k]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_grouped_npy_files(input_directory, output_directory):\n",
    "    \"\"\"Process each grouped npy file to replace padded values and save the modified files.\"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    npy_files = list_npy_files(input_directory)\n",
    "\n",
    "    for file_path in npy_files:\n",
    "        data = np.load(file_path)\n",
    "        modified_data = replace_padded_values(data)\n",
    "        output_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "        np.save(output_path, modified_data)\n",
    "        print(f\"Processed and saved {output_path}\")\n",
    "\n",
    "def main():\n",
    "    input_directory = '/data/tii/data/nuscenes_trainval_veh_norm_npy'\n",
    "    output_directory = '/data/tii/data/nuscenes_trainval_veh_norm_satur_npy'\n",
    "\n",
    "    process_grouped_npy_files(input_directory, output_directory)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add padding to seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "input_directory = '/data/tii/data/nuscenes_trainval_veh_processed_maxag_npy'\n",
    "output_directory = '/data/tii/data/nuscenes_trainval_veh_final_npy'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Define the desired sequence length after padding\n",
    "desired_length = 20\n",
    "\n",
    "# Function to pad sequences\n",
    "def pad_sequence(sequence, pad_len, position):\n",
    "    \"\"\"\n",
    "    Pad a sequence to the desired length.\n",
    "    \n",
    "    Args:\n",
    "        sequence (np.ndarray): The original sequence of shape (L, D).\n",
    "        pad_len (int): The total length to pad the sequence to.\n",
    "        position (str): Where to add padding. Options are 'beginning', 'middle', 'end', or 'beginning_end'.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The padded sequence of shape (pad_len, D).\n",
    "    \"\"\"\n",
    "    original_length = sequence.shape[0]\n",
    "    pad_amount = pad_len - original_length\n",
    "    pad_before = 0\n",
    "    pad_middle = 0\n",
    "    pad_after = 0\n",
    "    \n",
    "    if position == 'beginning':\n",
    "        pad_before = pad_amount\n",
    "    elif position == 'middle':\n",
    "        pad_middle = pad_amount\n",
    "    elif position == 'end':\n",
    "        pad_after = pad_amount\n",
    "    elif position == 'beginning_end':\n",
    "        pad_before = pad_amount // 2\n",
    "        pad_after = pad_amount - pad_before\n",
    "    else:\n",
    "        raise ValueError(\"Invalid position argument. Choose from 'beginning', 'middle', 'end', 'beginning_end'.\")\n",
    "\n",
    "    padding_before = np.zeros((pad_before, sequence.shape[1]))\n",
    "    padding_middle = np.zeros((pad_middle, sequence.shape[1]))\n",
    "    padding_after = np.zeros((pad_after, sequence.shape[1]))\n",
    "    \n",
    "    padded_sequence = np.vstack((padding_before, sequence[:sequence.shape[0]//2, :], padding_middle, sequence[sequence.shape[0]//2:, :], padding_after))\n",
    "    return padded_sequence\n",
    "\n",
    "# Loop through each file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.npy'):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        # Load the data\n",
    "        data = np.load(file_path)\n",
    "        \n",
    "        # Initialize the padded data array\n",
    "        padded_data = np.zeros((data.shape[0], desired_length, data.shape[2]))\n",
    "        \n",
    "        for i in range(data.shape[0]):\n",
    "            # Decide where to add padding ('beginning', 'middle', 'end', 'beginning_end')\n",
    "            positions = ['beginning', 'middle', 'end', 'beginning_end']\n",
    "            pos = positions[i % len(positions)]  # For illustration, cycle through the positions\n",
    "            \n",
    "            padded_data[i] = pad_sequence(data[i], desired_length, pos)\n",
    "        \n",
    "        # Save the padded data\n",
    "        output_path = os.path.join(output_directory, filename)\n",
    "        np.save(output_path, padded_data)\n",
    "\n",
    "print(\"Padding completed and saved to the output directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maps preprocessing"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "traffsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
